{"cells":[{"cell_type":"markdown","id":"0eb0215b","metadata":{},"source":["<img src=\"https://blobcity.com/assets/svg/logos/logo.svg\" alt=\"AI in the Cloud\" width=\"150\" height=\"50\">"]},{"cell_type":"markdown","id":"d6b56986","metadata":{},"source":["# Classification Problem\r\n"]},{"cell_type":"code","execution_count":null,"id":"60b360d7","metadata":{},"outputs":[],"source":["# imports\r","import numpy as np\r","import pandas as pd\r","import matplotlib.pyplot as plt\r","import seaborn as se\r","import warnings\r","from sklearn.model_selection import train_test_split\r","from sklearn.preprocessing import LabelEncoder\r","from sklearn.metrics import classification_report,plot_confusion_matrix\r","warnings.filterwarnings('ignore')\r\n","from sklearn.preprocessing import RobustScaler\r","from imblearn.under_sampling import InstanceHardnessThreshold\r","from sklearn.preprocessing import QuantileTransformer\r","from sklearn.ensemble import RandomForestClassifier\r\n"]},{"cell_type":"markdown","id":"2a57361b","metadata":{},"source":["### Data Fetch\n"," Pandas is an open-source, BSD-licensed library providing high-performance,easy-to-use data manipulation and data analysis tools."]},{"cell_type":"code","execution_count":null,"id":"73831aea","metadata":{},"outputs":[],"source":["# Data Fetch\r","file=''\r","df=pd.read_csv(file)\r","df.head()\r\n"]},{"cell_type":"markdown","id":"be69eb40","metadata":{},"source":["### Feature Selection\n"," It is the process of reducing the number of input variables when developing a predictive model.Used to reduce the number of input variables to reduce the computational cost of modelling and,in some cases,to improve the performance of the model."]},{"cell_type":"code","execution_count":null,"id":"9bf21803","metadata":{},"outputs":[],"source":["# Selected Columns\r","features=['LIST OF FEATURES/COLUMN NAMES']\r","target='TARGET COLUMN NAME'\r\n","# X & Y\r","X=df[features]\r","Y=df[target]\r\n"]},{"cell_type":"markdown","id":"48d308e8","metadata":{},"source":["### Data Encoding\n"," Converting the string classes data in the datasets by encoding them to integer either using OneHotEncoding or LabelEncoding"]},{"cell_type":"code","execution_count":null,"id":"054c3e8c","metadata":{},"outputs":[],"source":["# Handling AlphaNumeric Features\r","X=pd.get_dummies(X)\r\n"]},{"cell_type":"markdown","id":"b16c4d23","metadata":{},"source":["### Correlation Matrix\n"," In order to check the correlation between the features, we will plot a correlation matrix. It is effective in summarizing a large amount of data where the goal is to see patterns."]},{"cell_type":"code","execution_count":null,"id":"6d11bda1","metadata":{},"outputs":[],"source":["f,ax = plt.subplots(figsize=(18, 18))\r","matrix = np.triu(X.corr())\r","se.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax, mask=matrix)\r","plt.show()\n"]},{"cell_type":"markdown","id":"6ee5438e","metadata":{},"source":["### Data Rescaling\n"," Feature scaling or Data scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization"]},{"cell_type":"code","execution_count":null,"id":"f065481a","metadata":{},"outputs":[],"source":["columns=X.columns\r","X=RobustScaler().fit_transform(X)\r","X=pd.DataFrame(data = X,columns = columns)\r","X.head()\r"]},{"cell_type":"markdown","id":"2956b31a","metadata":{},"source":["### Train & Test\n"," The train-test split is a procedure for evaluating the performance of an algorithm.The procedure involves taking a dataset and dividing it into two subsets.The first subset is utilized to fit/train the model.The second subset is used for prediction.The main motive is to estimate the performance of the model on new data."]},{"cell_type":"code","execution_count":null,"id":"0acea244","metadata":{},"outputs":[],"source":["# Data split for training and testing\r","X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=123)\r\n"]},{"cell_type":"markdown","id":"8a0267b1","metadata":{},"source":["### Target Balancing\r","InstanceHardnessThreshold is a specific algorithm in which a classifier is trained on the data and the samples with lower probabilities are removed"]},{"cell_type":"code","execution_count":null,"id":"4e40ca97","metadata":{},"outputs":[],"source":["# resampling target\r","resample=InstanceHardnessThreshold()\r","X_train,Y_train=resample.fit_resample(X_train,Y_train)"]},{"cell_type":"markdown","id":"532059cc","metadata":{},"source":["### Feature Transformation\n","  Feature transformation is a mathematical transformation in which we apply a mathematical formula to data and transform the values which are useful for our further analysis."]},{"cell_type":"code","execution_count":null,"id":"5fe7999c","metadata":{},"outputs":[],"source":["quantiletransformer=QuantileTransformer()\r","X_train=quantiletransformer.fit_transform(X_train)\r","X_test=quantiletransformer.transform(X_test)"]},{"cell_type":"markdown","id":"7282b574","metadata":{},"source":["### Model\n","\n","A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the <code>max_samples</code> parameter if <code>bootstrap=True</code> (default), otherwise the whole dataset is used to build each tree.\n","\n","#### Model Tuning Parameters\n","\n","1. n_estimators : The number of trees in the forest.\n","\n","2. criterion : The function to measure the quality of a split. Supported criteria are 'gini' for the Gini impurity and 'entropy' for the information gain.\n","\n","3. max_depth : The maximum depth of the tree.\n","\n","4. max_features : The number of features to consider when looking for the best split:\n","\n","5. bootstrap : Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n","\n","6. oob_score : Whether to use out-of-bag samples to estimate the generalization accuracy.\n"]},{"cell_type":"code","execution_count":null,"id":"9e4ea9c8","metadata":{},"outputs":[],"source":["# Model Initialization\r","model=RandomForestClassifier()\r","model.fit(X_train,Y_train)\r\n"]},{"cell_type":"markdown","id":"4f0dae4e","metadata":{},"source":["### Accuracy Metrics\n"," Performance metrics are a part of every machine learning pipeline. They tell you if you're making progress, and put a number on it. All machine learning models,whether it's linear regression, or a SOTA technique like BERT, need a metric to judge performance."]},{"cell_type":"code","execution_count":null,"id":"58936697","metadata":{},"outputs":[],"source":["# Confusion Matrix\r","plot_confusion_matrix(model,X_test,Y_test,cmap=plt.cm.Blues)\r\n","# Classification Report\r","print(classification_report(Y_test,model.predict(X_test)))\r\n"]}],"metadata":{},"nbformat":4,"nbformat_minor":5}